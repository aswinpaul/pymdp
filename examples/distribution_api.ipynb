{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Distributions API\n",
    "\n",
    "In this notebook we'll give some example uses of the named distribution api\n",
    "designed for easier querying and construction of complicated A and B tensors.\n",
    "\n",
    "The distribution objects allow for giving semantically sensible names to axes\n",
    "and indices within a tensor. These can be made interactively in code or an \n",
    "entire set of A and B tensors can be compiled from a structured model\n",
    "description.\n",
    "\n",
    "Below is an example of how to build a distribution from code for a model\n",
    "conisting of a single observation modality \"observation\" consiting of the\n",
    "possible observations {A, B, C, D}. A hidden state \"state\" consisting of the\n",
    "values {A, B, C, D} and controls \"control\" {up, down}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "from pymdp.jax import Distribution\n",
    "from pymdp.jax.agent import Agent\n",
    "\n",
    "def get_task_info():\n",
    "    policies = jnp.expand_dims(jnp.array([[0, 0, 0, 0], [1, 1, 1, 1]]), -1)\n",
    "    C = jnp.zeros((1, 4))\n",
    "    C = C.at[0, 3].set(1.0)\n",
    "    action = jnp.array([[1]])\n",
    "    qs = [jnp.zeros((1, 1, 4))]\n",
    "    qs[0] = qs[0].at[0, 0, 0].set(1.0)\n",
    "    observation = [jnp.array([[0]])]\n",
    "    return policies, C, action, qs, observation\n",
    "\n",
    "observations = [\"A\", \"B\", \"C\", \"D\"]\n",
    "states = [\"A\", \"B\", \"C\", \"D\"]\n",
    "controls = [\"up\", \"down\"]\n",
    "\n",
    "data = np.zeros((len(observations), len(states)))\n",
    "A = Distribution(data, {\"observations\": observations}, {\"states\": states})\n",
    "\n",
    "A[\"A\", \"A\"] = 1.0\n",
    "A[\"B\", \"B\"] = 1.0\n",
    "A[\"C\", \"C\"] = 1.0\n",
    "A[\"D\", \"D\"] = 1.0\n",
    "\n",
    "data = np.zeros((len(states), len(states), len(controls)))\n",
    "B = Distribution(data, {\"states\": states}, {\"states\": states, \"controls\": controls})\n",
    "\n",
    "B[\"B\", \"A\", \"up\"] = 1.0\n",
    "B[\"C\", \"B\", \"up\"] = 1.0\n",
    "B[\"D\", \"C\", \"up\"] = 1.0\n",
    "B[\"D\", \"D\", \"up\"] = 1.0\n",
    "\n",
    "B[\"A\", \"A\", \"down\"] = 1.0\n",
    "B[\"A\", \"B\", \"down\"] = 1.0\n",
    "B[\"B\", \"C\", \"down\"] = 1.0\n",
    "B[\"C\", \"D\", \"down\"] = 1.0\n",
    "\n",
    "policies, C, action, qs, observation = get_task_info()\n",
    "\n",
    "agent = Agent([A], [B], [C], policies=policies)\n",
    "prior, _ = agent.infer_empirical_prior(action, qs)\n",
    "qs = agent.infer_states(observation, None, prior, None)\n",
    "\n",
    "q_pi, G = agent.infer_policies(qs)\n",
    "action = agent.sample_action(q_pi)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using configs\n",
    "Alternatively you can use a model description to just generate the shape of the\n",
    "A's and the B's in one go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]]\n"
     ]
    }
   ],
   "source": [
    "from pymdp.jax import distribution\n",
    "\n",
    "model = {\n",
    "    \"observations\": {\n",
    "        \"o1\": {\"elements\": [\"A\", \"B\", \"C\", \"D\"], \"depends_on\": [\"s1\"]},\n",
    "    },\n",
    "    \"controls\": {\"c1\": {\"elements\": [\"up\", \"down\"]}},\n",
    "    \"states\": {\n",
    "        \"s1\": {\"elements\": [\"A\", \"B\", \"C\", \"D\"], \"depends_on\": [\"s1\"], \"controlled_by\": [\"c1\"]},\n",
    "    },\n",
    "}\n",
    "\n",
    "As, Bs = distribution.compile_model(model)\n",
    "\n",
    "As[0][\"A\", \"A\"] = 1.0\n",
    "As[0][\"B\", \"B\"] = 1.0\n",
    "As[0][\"C\", \"C\"] = 1.0\n",
    "As[0][\"D\", \"D\"] = 1.0\n",
    "\n",
    "Bs[0][\"B\", \"A\", \"up\"] = 1.0\n",
    "Bs[0][\"C\", \"B\", \"up\"] = 1.0\n",
    "Bs[0][\"D\", \"C\", \"up\"] = 1.0\n",
    "Bs[0][\"D\", \"D\", \"up\"] = 1.0\n",
    "\n",
    "Bs[0][\"A\", \"A\", \"down\"] = 1.0\n",
    "Bs[0][\"A\", \"B\", \"down\"] = 1.0\n",
    "Bs[0][\"B\", \"C\", \"down\"] = 1.0\n",
    "Bs[0][\"C\", \"D\", \"down\"] = 1.0\n",
    "\n",
    "policies, Cs, action, qs, observation = get_task_info()\n",
    "\n",
    "agent = Agent(As, Bs, Cs, policies=policies)\n",
    "prior, _ = agent.infer_empirical_prior(action, qs)\n",
    "qs = agent.infer_states(observation, None, prior, None)\n",
    "\n",
    "q_pi, G = agent.infer_policies(qs)\n",
    "action = agent.sample_action(q_pi)\n",
    "print(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
