{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized T-Maze environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pymdp.jax.envs.generalized_tmaze import (\n",
    "    GeneralizedTMazeEnv, parse_maze, render \n",
    ")\n",
    "from pymdp.jax.envs.rollout import rollout\n",
    "from pymdp.jax.agent import Agent\n",
    "\n",
    "import numpy as np \n",
    "import jax.random as jr \n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the environment\n",
    "\n",
    "In this example we create a simple square environment, where multiple cues are present, and multiple reward pairs. Each cue indicates the location of one of the reward pairs. \n",
    "\n",
    "The agent is can move in the grid world using actions up, down, left and right, and observes the current tile it is at. \n",
    "\n",
    "The grid world is specified by a matrix using the following labels: \n",
    "\n",
    "```\n",
    "0: Empty space\n",
    "1: The initial position of the agent\n",
    "2: Walls\n",
    "3 + i: Cue for reward i\n",
    "4 + i: Potential reward location i 1\n",
    "4 + i: Potential reward location i 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maze_matrix(small=False):\n",
    "    if small:\n",
    "        M = np.zeros((3, 5))\n",
    "\n",
    "        # Set the reward locations\n",
    "        M[0,1] = 4\n",
    "        M[1,1] = 5\n",
    "        M[1,3] = 7\n",
    "        M[0,3] = 8\n",
    "\n",
    "        # Set the cue locations\n",
    "        M[2,0] = 3\n",
    "        M[2,4] = 6\n",
    "\n",
    "        # Set the initial position\n",
    "        M[2,3] = 1\n",
    "    else:\n",
    "\n",
    "        M = np.zeros((5, 5))\n",
    "\n",
    "        # Set the reward locations\n",
    "        M[0,1] = 4\n",
    "        M[1,1] = 5\n",
    "        M[1,3] = 7\n",
    "        M[0,3] = 8\n",
    "        M[4,1] = 10\n",
    "        M[4,3] = 11\n",
    "\n",
    "        # Set the cue locations\n",
    "        M[2,0] = 3\n",
    "        M[2,4] = 6\n",
    "        M[3,2] = 9\n",
    "\n",
    "        # Set the initial position\n",
    "        M[2,2] = 1\n",
    "    return M\n",
    "\n",
    "M = get_maze_matrix(small=False)\n",
    "env_info = parse_maze(M)\n",
    "tmaze_env = GeneralizedTMazeEnv(env_info)\n",
    "_ = render(env_info, tmaze_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the agent. \n",
    "\n",
    "The PyMDPEnv class consists of a params dict that contains the A, B, and D vectors of the environment. We initialize our agent using the same parameters. This means that the agent has full knowledge about the environment transitions, and likelihoods. We initialize the agent with a flat prior, i.e. it does not know where it, or the reward is. Finally, we set the C vector to have a preference only over the rewarding observation of cue-reward pair 1 (i.e. C[1][1] = 1 and zero for other values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [a.copy() for a in tmaze_env.params[\"A\"]]\n",
    "B = [b.copy() for b in tmaze_env.params[\"B\"]]\n",
    "A_dependencies = tmaze_env.dependencies[\"A\"]\n",
    "B_dependencies = tmaze_env.dependencies[\"B\"]\n",
    "\n",
    "# [position], [cue], [reward]\n",
    "C = [jnp.zeros(a.shape[:2]) for a in A]\n",
    "\n",
    "rewarding_modality = 2 + env_info[\"num_cues\"]\n",
    "#rewarding_modality = -1\n",
    "\n",
    "C[rewarding_modality] = C[rewarding_modality].at[:,1].set(2.0)\n",
    "C[rewarding_modality] = C[rewarding_modality].at[:,2].set(-3.0)\n",
    "\n",
    "\n",
    "D = [jnp.ones(b.shape[:2]) for b in B]\n",
    "\n",
    "agent = Agent(\n",
    "    A, B, C, D, \n",
    "    None, None, None, \n",
    "    policy_len=7,\n",
    "    A_dependencies=A_dependencies, \n",
    "    B_dependencies=B_dependencies,\n",
    "    apply_batch=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout an agent episode \n",
    "\n",
    "Using the rollout function, we can run an active inference agent in this environment over a specified number of discrete timesteps using the parameters previously set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.PRNGKey(0)\n",
    "_, info, _ = rollout(agent, tmaze_env, num_timesteps=15, rng_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = []\n",
    "for t in range(15): \n",
    "    print(f'Time t={t}')\n",
    "    env_state = jtu.tree_map(lambda x: x[:, t], info['env'])\n",
    "    ims.append(render(env_info, env_state))\n",
    "ims = [np.array(i) for i in ims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio as io\n",
    "\n",
    "io.mimsave('tmp_dir/gif.gif', ims, format=\"GIF\", duration=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
